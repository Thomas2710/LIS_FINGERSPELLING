{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "312a572b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7f1c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import mediapipe as mp\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a48306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(image, hands):\n",
    "    # Process the image and get results\n",
    "    results = hands.process(image)\n",
    "\n",
    "    # Convert RGB back to BGR for OpenCV\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Get image dimensions\n",
    "            h, w, _ = image.shape\n",
    "\n",
    "            # Extract landmark positions\n",
    "            landmark_array = [(int(lm.x * w), int(lm.y * h)) for lm in hand_landmarks.landmark]\n",
    "            if landmark_array:\n",
    "\n",
    "                # Get bounding box coordinates\n",
    "                x_coords = [pt[0] for pt in landmark_array]\n",
    "                y_coords = [pt[1] for pt in landmark_array]\n",
    "\n",
    "                xmin, xmax = min(x_coords)-20, max(x_coords)+20\n",
    "                ymin, ymax = min(y_coords)-20, max(y_coords)+20\n",
    "\n",
    "                # Draw the bounding box\n",
    "                #cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "                \n",
    "                # Crop the bounding box from the image\n",
    "                cropped_image = image[ymin:ymax, xmin:xmax].copy()\n",
    "\n",
    "                # Resize the cropped image to a fixed size (e.g., 150x150)\n",
    "                #cropped_image = cv2.resize(cropped_image, (150, 150))\n",
    "                if cropped_image.size != 0:\n",
    "                    # Convert the cropped image to grayscale\n",
    "                    cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    # Surround the crop with dark gray pixels\n",
    "                    bordered_image = cv2.copyMakeBorder(\n",
    "                        cropped_image, 100, 100, 100, 100, cv2.BORDER_CONSTANT, value=1\n",
    "                    )\n",
    "\n",
    "                    # Resize the bordered image to ensure it is 150x150\n",
    "                    bordered_image = cv2.resize(bordered_image, (150, 150))\n",
    "\n",
    "                    # Make the hand region whiter\n",
    "                    '''\n",
    "                    bordered_image = cv2.normalize(\n",
    "                        bordered_image, None, alpha=200, beta=255, norm_type=cv2.NORM_MINMAX\n",
    "                    )\n",
    "                    '''\n",
    "                    \n",
    "\n",
    "                    # Display the processed image\n",
    "                    cv2.imshow(\"Cropped Hand with Border\", bordered_image) \n",
    "\n",
    "                    return bordered_image\n",
    "    else:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25edf041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=22):  # Change num_classes as needed\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # Conv Layer 1: Input (1, 150, 150) -> Output (32, 148, 148) -> Pool (32, 74, 74)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Conv Layer 2: Output (64, 72, 72) -> Pool (64, 36, 36)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Conv Layer 3: Output (128, 34, 34) -> Pool (128, 17, 17)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(128*17*17, 512)  # Flattened input size\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # Conv1 -> ReLU -> Pool\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # Conv2 -> ReLU -> Pool\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # Conv3 -> ReLU -> Pool\n",
    "\n",
    "        x = x.view(x.shape[0], -1)  # Flatten for fully connected layers\n",
    "        x = F.relu(self.fc1(x))  # Fully Connected Layer 1 -> ReLU\n",
    "        x = self.fc2(x)  # Output layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d45214d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744673220.187450   50847 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1744673220.201860   50911 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 19.1.1, DRM 3.59, 6.11.0-19-generic)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1744673220.265282   50893 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1744673220.301960   50895 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/thomas/.cache/pypoetry/virtualenvs/lis-L4bSUyJ--py3.12/lib/python3.12/site-packages/cv2/qt/plugins\"\n",
      "W0000 00:00:1744673224.260643   50897 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PATH = os.path.join(os.getcwd(),'..','..','archive', 'LIS-fingerspelling-dataset')\n",
    "SAVEPATH = os.path.join(os.getcwd(), 'checkpoint','cnn', 'weights')\n",
    "REVERSEPATH = os.path.join(os.getcwd(), 'checkpoint','cnn','reverselookup.pickle')\n",
    "\n",
    "\n",
    "with open(REVERSEPATH, 'rb') as handle:\n",
    "    reverselookup = pickle.load(handle)\n",
    "\n",
    "if not reverselookup:\n",
    "    print(\"Error: Could not load reverse lookup table.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "model = CNNModel(num_classes=len(reverselookup.keys()))\n",
    "model.load_state_dict(torch.load(SAVEPATH, weights_only=True))\n",
    "\n",
    "# Define the transform (same as for single-image classification)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7\n",
    ") as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert BGR to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Flip the image for a selfie-view display (optional)\n",
    "        image = cv2.flip(image, 1)\n",
    "\n",
    "        #Process image to make it look like dataset\n",
    "        image = process(image, hands)\n",
    "\n",
    "\n",
    "        input = transform(image).unsqueeze(0) # Add batch dim\n",
    "\n",
    "\n",
    "        # Predict\n",
    "        outputs = model(input)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        letter = reverselookup[predicted.item()]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # Display prediction\n",
    "        cv2.putText(frame, f\"Prediction: {letter}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "        cv2.imshow('Image Recognition', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lis-L4bSUyJ--py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
