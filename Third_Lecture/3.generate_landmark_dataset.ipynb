{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a1c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63fbb40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744673272.939297   50940 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1744673272.950153   51005 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 19.1.1, DRM 3.59, 6.11.0-19-generic)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1744673273.018119   50986 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1744673273.069812   50985 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1744673273.652907   50994 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted landmarks from images: 282 from 299 images in e\n",
      "Extracted landmarks from images: 62 from 309 images in q\n",
      "Extracted landmarks from images: 210 from 246 images in a\n",
      "Extracted landmarks from images: 55 from 283 images in p\n",
      "Extracted landmarks from images: 27 from 328 images in m\n",
      "Extracted landmarks from images: 267 from 312 images in x\n",
      "Extracted landmarks from images: 191 from 311 images in y\n",
      "Extracted landmarks from images: 32 from 274 images in n\n",
      "Extracted landmarks from images: 0 from 275 images in v\n",
      "Extracted landmarks from images: 240 from 254 images in c\n",
      "Extracted landmarks from images: 128 from 328 images in u\n",
      "Extracted landmarks from images: 273 from 281 images in f\n",
      "Extracted landmarks from images: 191 from 309 images in l\n",
      "Extracted landmarks from images: 246 from 272 images in o\n",
      "Extracted landmarks from images: 295 from 314 images in w\n",
      "Extracted landmarks from images: 224 from 299 images in h\n",
      "Extracted landmarks from images: 219 from 273 images in d\n",
      "Extracted landmarks from images: 202 from 287 images in t\n",
      "Extracted landmarks from images: 282 from 302 images in r\n",
      "Extracted landmarks from images: 11 from 307 images in k\n",
      "Extracted landmarks from images: 121 from 310 images in i\n",
      "Extracted landmarks from images: 92 from 255 images in b\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function to extract landmarks\n",
    "def extract_hand_landmarks(results):\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            landmarks = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            return landmarks\n",
    "    return None\n",
    "\n",
    "def center_landmarks(landmarks):\n",
    "    \"\"\"\n",
    "    Center and scale hand landmarks around the wrist (landmark 0).\n",
    "\n",
    "    Parameters:\n",
    "        landmarks (list or np.ndarray): Flat list or array of 63 values (21 landmarks * 3 coordinates)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed landmarks, same shape (63,)\n",
    "    \"\"\"\n",
    "    # Convert to numpy array\n",
    "    landmarks = np.array(landmarks)\n",
    "\n",
    "    # Reshape to (21, 3)\n",
    "    landmarks = landmarks.reshape((21, 3))\n",
    "\n",
    "    # Step 1: Centering - subtract wrist coordinates\n",
    "    wrist = landmarks[0]\n",
    "    centered = landmarks - wrist\n",
    "\n",
    "    # Step 2: Scaling - normalize by maximum distance from wrist\n",
    "    # Compute Euclidean distances from wrist to each point\n",
    "    distances = np.linalg.norm(centered, axis=1)\n",
    "    max_distance = distances.max()\n",
    "\n",
    "    # To avoid division by zero (if max_distance is 0)\n",
    "    if max_distance > 0:\n",
    "        scaled = centered / max_distance\n",
    "    else:\n",
    "        scaled = centered  # if hand is not detected well, just keep centered\n",
    "\n",
    "    # Flatten back to (63,)\n",
    "    return scaled.flatten()\n",
    "\n",
    "\n",
    "# Define MediaPipe hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "PATH = os.path.join(os.getcwd(),'archive', 'LIS-fingerspelling-dataset')\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7, max_num_hands=1) as hands:\n",
    "    for j in os.listdir(PATH):\n",
    "        count=0\n",
    "        if j != 'readme.txt': \n",
    "            for k in os.listdir(PATH+ '/'+str(j)):\n",
    "                path = PATH +'/'+ str(j)+'/'+str(k)\n",
    "                img = cv2.imread(path,cv2.IMREAD_COLOR)\n",
    "                results = hands.process(img)\n",
    "                landmarks = extract_hand_landmarks(results)\n",
    "                if landmarks:\n",
    "                    # Center and scale landmarks\n",
    "                    landmarks = center_landmarks(landmarks)\n",
    "                    count += 1\n",
    "                    #print('Extracting landmarks from images:', count, 'from', len(os.listdir(PATH+ '/'+str(j))), 'images in', str(j))\n",
    "                    tmp_path = os.path.join(os.getcwd(), 'archive', 'landmarks',str(j))+'/'+str(k)[:-4]+'.pkl'\n",
    "                    # Save\n",
    "                    with open(tmp_path, 'wb') as f:\n",
    "                        pickle.dump(landmarks, f)\n",
    "                    mp_drawing.draw_landmarks(img, results.multi_hand_landmarks[0], mp_hands.HAND_CONNECTIONS)\n",
    "                    tmp_path_imgs = os.path.join(os.getcwd(), 'archive', 'landmarked_images',str(j))+'/'+str(k)[:-4]+'.jpg'\n",
    "                    cv2.imwrite(tmp_path_imgs, img)\n",
    "            print('Extracted landmarks from images:', count, 'from', len(os.listdir(PATH+ '/'+str(j))), 'images in', str(j))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lis-L4bSUyJ--py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
