{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57c5510f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49969ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import pickle\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import csv\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522bec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_folder_path = os.path.join(os.getcwd(), 'checkpoint', 'landmark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36648052",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = pickle.load(open(os.path.join(checkpoints_folder_path, 'label_encoder.pkl'), 'rb'))\n",
    "num_classes = len(le.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c586e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. MLP Model ===\n",
    "\n",
    "class SignLanguageMLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SignLanguageMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09cc0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(checkpoints_folder_path,'best_model.pth')):\n",
    "    # Load the model\n",
    "    model = SignLanguageMLP(input_size=63, num_classes=num_classes)\n",
    "    model.load_state_dict(torch.load(os.path.join(checkpoints_folder_path, 'best_model.pth'), map_location=torch.device('cpu')))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc982f9",
   "metadata": {},
   "source": [
    "## UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20fd5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing function to extract landmarks\n",
    "def extract_hand_landmarks(results):\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            landmarks = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            return landmarks\n",
    "    return None\n",
    "\n",
    "def center_landmarks(landmarks):\n",
    "    \"\"\"\n",
    "    Center and scale hand landmarks around the wrist (landmark 0).\n",
    "\n",
    "    Parameters:\n",
    "        landmarks (list or np.ndarray): Flat list or array of 63 values (21 landmarks * 3 coordinates)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed landmarks, same shape (63,)\n",
    "    \"\"\"\n",
    "    # Convert to numpy array\n",
    "    landmarks = np.array(landmarks)\n",
    "\n",
    "    # Reshape to (21, 3)\n",
    "    landmarks = landmarks.reshape((21, 3))\n",
    "\n",
    "    # Step 1: Centering - subtract wrist coordinates\n",
    "    wrist = landmarks[0]\n",
    "    centered = landmarks - wrist\n",
    "\n",
    "    # Step 2: Scaling - normalize by maximum distance from wrist\n",
    "    # Compute Euclidean distances from wrist to each point\n",
    "    distances = np.linalg.norm(centered, axis=1)\n",
    "    max_distance = distances.max()\n",
    "\n",
    "    # To avoid division by zero (if max_distance is 0)\n",
    "    if max_distance > 0:\n",
    "        scaled = centered / max_distance\n",
    "    else:\n",
    "        scaled = centered  # if hand is not detected well, just keep centered\n",
    "\n",
    "    # Flatten back to (63,)\n",
    "    return scaled.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c03e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744562585.001270   20319 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1744562585.005143   20463 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 19.1.1, DRM 3.59, 6.11.0-19-generic)\n",
      "W0000 00:00:1744562585.030738   20451 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1744562585.066582   20452 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection finished!\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7, max_num_hands=2) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            landmarks = extract_hand_landmarks(results)\n",
    "            if landmarks:\n",
    "                # Center and scale landmarks\n",
    "                landmarks = center_landmarks(landmarks)\n",
    "                features = torch.tensor(landmarks, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "                key = cv2.waitKey(10) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    break\n",
    "                else:\n",
    "                    # Make prediction\n",
    "                    with torch.no_grad():\n",
    "                        output = model(features)\n",
    "                        _, predicted = torch.max(output, 1)\n",
    "                        predicted_label = predicted.item()\n",
    "                        predicted_label = le.inverse_transform([predicted_label])[0]\n",
    "                    # Display prediction\n",
    "                    cv2.putText(frame, f'Predicted: {predicted_label}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                    cv2.imshow('LIS alphabet', frame)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Data collection finished!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lis-L4bSUyJ--py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
